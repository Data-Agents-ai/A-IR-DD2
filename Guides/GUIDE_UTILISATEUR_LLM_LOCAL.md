# ü§ñ Guide Utilisateur: Utiliser un LLM Local (Ollama, LMStudio)

**Derni√®re mise √† jour**: 18 D√©cembre 2025  
**Compr√©hensibilit√©**: Utilisateur non-technique ‚≠ê‚≠ê‚òÜ  

---

## üéØ Qu'est-ce qu'un LLM Local?

Un **LLM Local** est une intelligence artificielle qui tourne sur **votre ordinateur** au lieu du cloud:

| Cloud LLM | LLM Local |
|---|---|
| OpenAI (ChatGPT) | Ollama |
| Google (Gemini) | LMStudio |
| Anthropic (Claude) | Jan AI |
| | Votre ordi üíª |

**Avantages LLM Local:**
- ‚úÖ Gratuit (pas de co√ªt API)
- ‚úÖ Priv√© (donn√©es restent chez vous)
- ‚úÖ Rapide (pas de latence r√©seau)
- ‚ö†Ô∏è Mod√®les plus petits (7B au lieu de 70B)

---

## üöÄ Setup en 3 √âtapes

### √âtape 1: Installer Ollama

**Windows:**
1. Allez sur https://ollama.ai
2. Cliquez "Download for Windows"
3. Installez et lancez
4. Terminal: `ollama run mistral` (t√©l√©charge le mod√®le)

**Mac:**
1. Allez sur https://ollama.ai
2. Cliquez "Download for Mac"
3. Installez et lancez

**Linux:**
```bash
curl https://ollama.ai/install.sh | sh
ollama run mistral
```

Ollama d√©marre sur **http://localhost:11434** (port par d√©faut)

### √âtape 2: Configurer dans A-IR-DD2

1. Ouvrez l'application A-IR-DD2
2. Cliquez **Settings** (‚öôÔ∏è en haut √† droite)
3. Trouvez section **"LLM local (on premise)"**
4. Entrez l'endpoint: `http://localhost:11434`
5. Cliquez **"Test & Save"**
6. Attendez ~15 secondes
7. Si ‚úÖ vert ‚Üí Configuration r√©ussie!

### √âtape 3: Cr√©er un Agent

1. Cliquez **"+Cr√©er Prototype"** (ou agent)
2. Choisissez **LLM local (on premise)** dans le dropdown
3. S√©lectionnez le mod√®le d√©tect√© (ex: "mistral")
4. Configurez votre agent (syst√®me prompt, etc.)
5. Cliquez **Sauvegarder**
6. Envoyez un message ‚Üí L'agent r√©pond! üéâ

---

## üìã Mod√®les Recommand√©s

### Pour Commencer
| Mod√®le | Taille | Vitesse | Qualit√© | Commande |
|---|---|---|---|---|
| **Mistral** | 7B | Rapide ‚ö° | Moyen | `ollama run mistral` |
| **Llama2** | 7B | Rapide ‚ö° | Moyen | `ollama run llama2` |
| **Qwen** | 7B | Rapide ‚ö° | Bon | `ollama run qwen:7b` |

### Pour D√©veloppement
| Mod√®le | Sp√©cialit√© | Commande |
|---|---|---|
| **Qwen Coder** | Code | `ollama run qwen2.5-coder:7b` |
| **Mistral v0.3** | G√©n√©ral + fonctions | `ollama run mistral:v0.3` |
| **Llama 3.2** | Vision + chat | `ollama run llama2-vision` |

### Pour Performance
| Mod√®le | Taille | RAM Utilis√©e | Commande |
|---|---|---|---|
| **Qwen 2B** | 2B | 4-6 GB | `ollama run qwen:2b` |
| **Phi 3** | 3B | 6-8 GB | `ollama run phi3` |
| **Qwen 7B** | 7B | 10-12 GB | `ollama run qwen:7b` |
| **Mistral 7B** | 7B | 10-12 GB | `ollama run mistral` |
| **Llama 13B** | 13B | 16-20 GB | `ollama run llama2:13b` |

**RAM Requise**: Mod√®le + Syst√®me + Buffer ‚âà 2x taille mod√®le

---

## üîß D√©pannage

### ‚ùå "Connection refused" ou "Cannot reach endpoint"

**Cause**: Ollama/LMStudio ne tourne pas

**Solution:**
1. V√©rifiez que Ollama est lanc√© (ic√¥ne dans taskbar)
2. Terminal: `ollama serve` pour d√©marrer manuellement
3. V√©rifiez le port: `http://localhost:11434` (Ollama) ou `http://localhost:3928` (LMStudio)

### ‚ùå "No models available"

**Cause**: Ollama lanc√© mais aucun mod√®le t√©l√©charg√©

**Solution:**
```bash
ollama run mistral  # T√©l√©charge et lance Mistral 7B
```

Attendez le t√©l√©chargement (~3-5 GB, ~10 min selon connexion)

### ‚ùå L'agent r√©pond tr√®s lentement

**Cause 1**: Mod√®le trop gros pour votre RAM
- Solution: Utilisez Qwen 2B ou Phi 3 (plus l√©gers)

**Cause 2**: Manque de RAM disponible
- Solution: Fermez autres applications, augmentez RAM si possible

### ‚ùå Le port 11434 est occup√©

**Cause**: Ollama d√©j√† lanc√© elsewhere, ou autre service

**Solution:**
```bash
# Changez le port Ollama (exemple: 11435)
OLLAMA_HOST=localhost:11435 ollama serve

# Puis configurez A-IR-DD2 avec: http://localhost:11435
```

---

## üí° Cas d'Usage

### 1. D√©veloppeur: Code Generation
```
LLM: Qwen 2.5 Coder 7B
Prompt: "√âcris une fonction Python qui..."
‚Üí R√©ponse: Code complet + explications
```

### 2. Writer: Brainstorming
```
LLM: Mistral 7B
Prompt: "Donne-moi 5 id√©es de titres pour..."
‚Üí R√©ponse: Liste cr√©ative
```

### 3. Analyseur: Parsing Data
```
LLM: Llama 3.2
Prompt: "Parse ce JSON et extrait..."
Capacit√©: JSON mode ‚Üí Output structur√©
```

---

## ‚ö° Tips & Tricks

### 1. Multi-Mod√®les
Vous pouvez avoir plusieurs mod√®les et **les charger tous**:
```bash
# Terminal 1
ollama run mistral

# Terminal 2
ollama run qwen:7b

# Terminal 3
ollama run llama2
```
Puis dans A-IR-DD2, testez ‚Üí Tous s'affichent! üéØ

### 2. Optimiser Performance
- Fermez navigateur / autres apps (lib√®re RAM)
- Utilisez Qwen 2B si vous avez <8GB RAM
- Activez GPU dans Ollama pour plus de vitesse

### 3. Am√©liorer Qualit√©
- Utiliser Mistral au lieu de Qwen pour meilleure r√©ponse
- Augmenter `temperature` (plus cr√©atif) ou diminuer (plus focus)
- Ajouter des exemples dans le syst√®me prompt

### 4. D√©boguer une Mauvaise R√©ponse
1. Testez directement: `curl http://localhost:11434/v1/chat/completions`
2. V√©rifiez le mod√®le s√©lectionn√©
3. V√©rifiez le syst√®me prompt
4. Essayez un mod√®le diff√©rent

---

## üìä Comparaison: Local vs Cloud

| Aspect | Local (Ollama) | Cloud (OpenAI) |
|---|---|---|
| **Co√ªt** | Gratuit | 0.003-0.03 $/msg |
| **Latence** | 100-500ms | 1-5 sec (r√©seau) |
| **Priv√©** | Oui ‚úÖ | Non (envoy√© √† OpenAI) |
| **Qualit√©** | Moyen (7B) | Excellent (70B+) |
| **RAM Requis** | 8-16GB | 0 GB (cloud) |
| **Hors-ligne** | Oui ‚úÖ | Non (besoin internet) |
| **Fonctions** | Basique | Avanc√©es (vision, etc.) |

**Quand utiliser Local?**
- Budget limit√©
- Donn√©es sensibles
- D√©veloppement / prototypage
- Hors-ligne

**Quand utiliser Cloud?**
- Besoin de meilleure qualit√©
- Vision / multimodal avanc√©
- Pas de serveur local
- Haute charge (pas de RAM limites)

---

## üéì Apprendre Plus

### Resources
- **Ollama Docs**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Model Library**: https://ollama.ai/library
- **LMStudio**: https://lmstudio.ai
- **Jan AI**: https://jan.ai

### Community
- Ollama Discord: https://discord.gg/ollama
- Reddit: r/LocalLLaMA

---

## ‚úÖ Checklist Setup

- [ ] Ollama install√© et lanc√©
- [ ] Mod√®le t√©l√©charg√© (`ollama run mistral`)
- [ ] A-IR-DD2 configur√© avec endpoint
- [ ] Test & Save ‚úÖ succ√®s
- [ ] Agent cr√©√© avec LLM local
- [ ] Premier message envoy√© ‚Üí r√©ponse re√ßue ‚úÖ

---

**Succ√®s! Vous utilisez un LLM local! üéâ**

Questions? Consultez le [Guide Technique](../technique/local_llm/ARCHITECTURE_OPTION_C_HYBRID.md) ou demandez √† votre √©quipe.
