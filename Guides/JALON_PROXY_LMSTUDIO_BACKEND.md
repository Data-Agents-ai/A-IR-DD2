# üîí Plan d'Impl√©mentation : Backend Proxy LMStudio

**Version**: 1.0  
**Date**: 2025-11-26  
**Responsable**: ARC-1  
**Statut**: üü¢ EN COURS

---

## üéØ OBJECTIF

Impl√©menter un **proxy backend s√©curis√©** entre le frontend React et le serveur LMStudio local pour r√©soudre les probl√®mes CORS et √©tablir une architecture SOLID.

### Probl√®me Initial
- ‚ùå Frontend appelle directement LMStudio (`http://localhost:1234`)
- ‚ùå Erreurs CORS bloquent toutes les requ√™tes
- ‚ùå Pas de contr√¥le/s√©curit√© sur les appels LLM
- ‚ùå Exposition directe du serveur local

### Solution Architecture
```
Frontend (React)
    ‚Üì HTTP/HTTPS
Backend Node/Express (Port 3001) ‚Üê PROXY S√âCURIS√â
    ‚Üì HTTP (localhost only)
LMStudio Server (Port 1234)
```

---

## üìä JALONS D'IMPL√âMENTATION

### ‚úÖ Jalon 0 : Pr√©paration (FAIT)
- [x] Analyse architecture existante
- [x] Validation plan par Chef de Projet
- [x] Documentation cr√©√©e

---

### üîµ Jalon 1 : Backend Proxy Foundation (2-3h)

**Objectif** : Cr√©er la structure backend et les routes de base

#### 1.1 Structure de fichiers √† cr√©er

```
backend/src/
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îî‚îÄ‚îÄ lmstudio.routes.ts          # Routes proxy LMStudio
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ lmstudioProxy.service.ts    # Logique m√©tier proxy
‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îú‚îÄ‚îÄ validateLMStudioRequest.ts  # Validation requ√™tes
‚îÇ   ‚îî‚îÄ‚îÄ rateLimiter.ts              # Rate limiting
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ lmstudio.config.ts          # Configuration endpoints
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ lmstudio.types.ts           # Types TypeScript
```

#### 1.2 Fichiers √† modifier
- `backend/src/server.ts` - Importer nouvelles routes

#### 1.3 T√¢ches d√©taill√©es

##### Task 1.1 : Configuration (`config/lmstudio.config.ts`)
```typescript
export const LMSTUDIO_CONFIG = {
  // Endpoints autoris√©s (whitelist)
  ALLOWED_ENDPOINTS: [
    'http://localhost:1234',
    'http://localhost:3928',      // Jan
    'http://127.0.0.1:1234',
    'http://127.0.0.1:3928'
  ],
  
  // Timeout s√©curis√©
  TIMEOUT_MS: 30000,
  
  // Rate limiting
  MAX_REQUESTS_PER_MINUTE: 60,
  
  // Taille max requ√™te (prevent DoS)
  MAX_REQUEST_SIZE: '10mb',
  
  // Cache TTL
  MODELS_CACHE_TTL_MS: 600000, // 10 minutes
  
  // Retry logic
  MAX_RETRIES: 3,
  RETRY_DELAY_MS: 1000
};
```

##### Task 1.2 : Types (`types/lmstudio.types.ts`)
```typescript
export interface LMStudioEndpointConfig {
  endpoint: string;
  timeout?: number;
}

export interface LMStudioHealthResponse {
  healthy: boolean;
  endpoint?: string;
  models?: number;
  error?: string;
}

export interface LMStudioModelResponse {
  id: string;
  object: string;
  created: number;
  owned_by: string;
}

export interface LMStudioModelsListResponse {
  data: LMStudioModelResponse[];
  object: string;
}
```

##### Task 1.3 : Service Proxy (`services/lmstudioProxy.service.ts`)
```typescript
import { LMSTUDIO_CONFIG } from '../config/lmstudio.config';
import type { LMStudioHealthResponse, LMStudioModelsListResponse } from '../types/lmstudio.types';

// Validation endpoint whitelist
export function isEndpointAllowed(endpoint: string): boolean {
  return LMSTUDIO_CONFIG.ALLOWED_ENDPOINTS.includes(endpoint);
}

// Fetch avec timeout
export async function fetchWithTimeout(
  url: string,
  options: RequestInit,
  timeout: number = LMSTUDIO_CONFIG.TIMEOUT_MS
): Promise<Response> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), timeout);
  
  try {
    const response = await fetch(url, {
      ...options,
      signal: controller.signal
    });
    clearTimeout(timeoutId);
    return response;
  } catch (error) {
    clearTimeout(timeoutId);
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error('Request timeout');
    }
    throw error;
  }
}

// Health check LMStudio
export async function checkLMStudioHealth(endpoint: string): Promise<LMStudioHealthResponse> {
  if (!isEndpointAllowed(endpoint)) {
    return { healthy: false, error: 'Endpoint not allowed' };
  }
  
  try {
    const response = await fetchWithTimeout(`${endpoint}/v1/models`, {
      method: 'GET',
      headers: { 'Content-Type': 'application/json' }
    }, 5000);
    
    if (!response.ok) {
      return { healthy: false, error: `HTTP ${response.status}` };
    }
    
    const data: LMStudioModelsListResponse = await response.json();
    return {
      healthy: true,
      endpoint,
      models: data.data?.length || 0
    };
  } catch (error) {
    return {
      healthy: false,
      error: error instanceof Error ? error.message : 'Unknown error'
    };
  }
}

// Fetch models depuis LMStudio
export async function fetchLMStudioModels(endpoint: string): Promise<LMStudioModelsListResponse> {
  if (!isEndpointAllowed(endpoint)) {
    throw new Error('Endpoint not allowed');
  }
  
  const response = await fetchWithTimeout(`${endpoint}/v1/models`, {
    method: 'GET',
    headers: { 'Content-Type': 'application/json' }
  });
  
  if (!response.ok) {
    throw new Error(`LMStudio API error: ${response.status}`);
  }
  
  return await response.json();
}

// Auto-detect endpoint disponible
export async function detectAvailableEndpoint(): Promise<string> {
  for (const endpoint of LMSTUDIO_CONFIG.ALLOWED_ENDPOINTS) {
    try {
      const health = await checkLMStudioHealth(endpoint);
      if (health.healthy) {
        return endpoint;
      }
    } catch {
      // Continue to next endpoint
    }
  }
  
  throw new Error('No LMStudio server detected');
}
```

##### Task 1.4 : Routes (`routes/lmstudio.routes.ts`)
```typescript
import { Router, Request, Response } from 'express';
import {
  checkLMStudioHealth,
  fetchLMStudioModels,
  detectAvailableEndpoint
} from '../services/lmstudioProxy.service';

const router = Router();

// GET /api/lmstudio/health
// Health check du serveur LMStudio
router.get('/health', async (req: Request, res: Response) => {
  try {
    const endpoint = req.query.endpoint as string || 'http://localhost:1234';
    const health = await checkLMStudioHealth(endpoint);
    
    res.json(health);
  } catch (error) {
    res.status(500).json({
      healthy: false,
      error: error instanceof Error ? error.message : 'Unknown error'
    });
  }
});

// GET /api/lmstudio/models
// R√©cup√©rer la liste des mod√®les disponibles
router.get('/models', async (req: Request, res: Response) => {
  try {
    const endpoint = req.query.endpoint as string || 'http://localhost:1234';
    const models = await fetchLMStudioModels(endpoint);
    
    res.json(models);
  } catch (error) {
    res.status(500).json({
      error: error instanceof Error ? error.message : 'Failed to fetch models'
    });
  }
});

// GET /api/lmstudio/detect-endpoint
// Auto-d√©tecter l'endpoint LMStudio disponible
router.get('/detect-endpoint', async (req: Request, res: Response) => {
  try {
    const endpoint = await detectAvailableEndpoint();
    res.json({ endpoint, detected: true });
  } catch (error) {
    res.status(404).json({
      detected: false,
      error: error instanceof Error ? error.message : 'No server detected'
    });
  }
});

export default router;
```

##### Task 1.5 : Int√©gration dans `server.ts`
```typescript
// Ajouter apr√®s les imports existants
import lmstudioRoutes from './routes/lmstudio.routes';

// Ajouter apr√®s les routes existantes
app.use('/api/lmstudio', lmstudioRoutes);
```

#### 1.4 Tests √† effectuer

**Test 1 : Health Check**
```powershell
# Backend doit √™tre d√©marr√©
curl http://localhost:3001/api/lmstudio/health

# R√©ponse attendue
{
  "healthy": true,
  "endpoint": "http://localhost:1234",
  "models": 2
}
```

**Test 2 : Liste Mod√®les**
```powershell
curl http://localhost:3001/api/lmstudio/models

# R√©ponse attendue
{
  "data": [
    {"id": "mistral-7b-instruct", "object": "model", ...},
    {"id": "qwen2.5-coder-7b", "object": "model", ...}
  ],
  "object": "list"
}
```

**Test 3 : Auto-d√©tection**
```powershell
curl http://localhost:3001/api/lmstudio/detect-endpoint

# R√©ponse attendue
{
  "endpoint": "http://localhost:1234",
  "detected": true
}
```

#### 1.5 Crit√®res de validation
- [ ] ‚úÖ Structure fichiers cr√©√©e
- [ ] ‚úÖ Routes `/health`, `/models`, `/detect-endpoint` fonctionnelles
- [ ] ‚úÖ Validation whitelist endpoints
- [ ] ‚úÖ Timeout 30s respect√©
- [ ] ‚úÖ 3 tests manuels passent
- [ ] ‚úÖ Aucune erreur TypeScript
- [ ] ‚úÖ Backend d√©marre sans erreur

---

### üîµ Jalon 2 : Streaming & Chat (3-4h)

**Objectif** : Impl√©menter le streaming des r√©ponses LLM via SSE

#### 2.1 T√¢ches

##### Task 2.1 : Types streaming
```typescript
// types/lmstudio.types.ts - Ajouter
export interface ChatCompletionRequest {
  model: string;
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  stream?: boolean;
  temperature?: number;
  max_tokens?: number;
  tools?: any[];
}

export interface ChatCompletionChunk {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    delta: {
      content?: string;
      tool_calls?: any[];
    };
    finish_reason: string | null;
  }>;
}
```

##### Task 2.2 : Service streaming
```typescript
// services/lmstudioProxy.service.ts - Ajouter
export async function* streamChatCompletion(
  endpoint: string,
  requestBody: ChatCompletionRequest
): AsyncGenerator<string, void, unknown> {
  if (!isEndpointAllowed(endpoint)) {
    throw new Error('Endpoint not allowed');
  }
  
  const response = await fetchWithTimeout(`${endpoint}/v1/chat/completions`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ ...requestBody, stream: true })
  });
  
  if (!response.ok) {
    throw new Error(`LMStudio error: ${response.status}`);
  }
  
  const reader = response.body?.getReader();
  if (!reader) throw new Error('No reader available');
  
  const decoder = new TextDecoder();
  
  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const chunk = decoder.decode(value);
      yield chunk; // Stream vers frontend
    }
  } finally {
    reader.releaseLock();
  }
}
```

##### Task 2.3 : Route streaming
```typescript
// routes/lmstudio.routes.ts - Ajouter
router.post('/chat/completions', async (req: Request, res: Response) => {
  try {
    const endpoint = req.body.endpoint || 'http://localhost:1234';
    const requestBody: ChatCompletionRequest = req.body;
    
    // Configuration SSE
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');
    
    // Stream depuis LMStudio vers frontend
    for await (const chunk of streamChatCompletion(endpoint, requestBody)) {
      res.write(chunk);
    }
    
    res.end();
  } catch (error) {
    res.status(500).json({
      error: error instanceof Error ? error.message : 'Streaming failed'
    });
  }
});
```

#### 2.2 Tests
```powershell
# Test streaming
curl -X POST http://localhost:3001/api/lmstudio/chat/completions `
  -H "Content-Type: application/json" `
  -d '{
    "model": "mistral-7b-instruct",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": true
  }'
```

#### 2.3 Crit√®res validation
- [ ] ‚úÖ Route `/chat/completions` fonctionnelle
- [ ] ‚úÖ Streaming SSE op√©rationnel
- [ ] ‚úÖ Gestion erreurs robuste
- [ ] ‚úÖ Test streaming manuel r√©ussi

---

### üîµ Jalon 3 : S√©curit√© (2h)

**Objectif** : Ajouter validation, rate limiting, logging

#### 3.1 T√¢ches

##### Task 3.1 : Middleware validation
```typescript
// middleware/validateLMStudioRequest.ts
import { Request, Response, NextFunction } from 'express';

export function validateChatRequest(
  req: Request,
  res: Response,
  next: NextFunction
) {
  const { model, messages } = req.body;
  
  if (!model || typeof model !== 'string') {
    return res.status(400).json({ error: 'Invalid model parameter' });
  }
  
  if (!Array.isArray(messages) || messages.length === 0) {
    return res.status(400).json({ error: 'Invalid messages array' });
  }
  
  // Validation messages
  for (const msg of messages) {
    if (!msg.role || !['system', 'user', 'assistant'].includes(msg.role)) {
      return res.status(400).json({ error: 'Invalid message role' });
    }
    if (typeof msg.content !== 'string') {
      return res.status(400).json({ error: 'Invalid message content' });
    }
  }
  
  next();
}

export function validateEndpoint(
  req: Request,
  res: Response,
  next: NextFunction
) {
  const endpoint = req.body.endpoint || req.query.endpoint as string;
  
  if (endpoint && !endpoint.startsWith('http://localhost') && !endpoint.startsWith('http://127.0.0.1')) {
    return res.status(403).json({ error: 'Endpoint must be localhost' });
  }
  
  next();
}
```

##### Task 3.2 : Rate limiting
```typescript
// middleware/rateLimiter.ts
import rateLimit from 'express-rate-limit';
import { LMSTUDIO_CONFIG } from '../config/lmstudio.config';

export const lmstudioRateLimiter = rateLimit({
  windowMs: 60 * 1000, // 1 minute
  max: LMSTUDIO_CONFIG.MAX_REQUESTS_PER_MINUTE,
  message: {
    error: 'Too many requests to LMStudio, please try again later'
  },
  standardHeaders: true,
  legacyHeaders: false,
});
```

##### Task 3.3 : Logging
```typescript
// middleware/logger.ts
import { Request, Response, NextFunction } from 'express';

export function logLMStudioRequest(
  req: Request,
  res: Response,
  next: NextFunction
) {
  const logEntry = {
    timestamp: new Date().toISOString(),
    method: req.method,
    path: req.path,
    ip: req.ip,
    userAgent: req.get('user-agent'),
    // Ne PAS logger: messages content (privacy)
  };
  
  console.log('[LMStudio Proxy]', JSON.stringify(logEntry));
  next();
}
```

##### Task 3.4 : Application middlewares
```typescript
// routes/lmstudio.routes.ts - Modifier
import { lmstudioRateLimiter } from '../middleware/rateLimiter';
import { validateChatRequest, validateEndpoint } from '../middleware/validateLMStudioRequest';
import { logLMStudioRequest } from '../middleware/logger';

// Appliquer √† toutes les routes
router.use(lmstudioRateLimiter);
router.use(logLMStudioRequest);
router.use(validateEndpoint);

// Route chat avec validation sp√©cifique
router.post('/chat/completions', validateChatRequest, async (req, res) => {
  // ... code existant
});
```

#### 3.2 Installation d√©pendances
```powershell
cd backend
npm install express-rate-limit
```

#### 3.3 Crit√®res validation
- [ ] ‚úÖ Validation requ√™tes fonctionnelle
- [ ] ‚úÖ Rate limiting 60 req/min appliqu√©
- [ ] ‚úÖ Logging sans fuite donn√©es sensibles
- [ ] ‚úÖ Whitelist endpoints respect√©e
- [ ] ‚úÖ Tests validation (requ√™tes invalides bloqu√©es)

---

### üîµ Jalon 4 : Frontend Migration (2h)

**Objectif** : Migrer frontend pour utiliser backend proxy

#### 4.1 T√¢ches

##### Task 4.1 : Configuration API
```typescript
// config/api.config.ts - CR√âER
export const API_CONFIG = {
  BACKEND_URL: import.meta.env.VITE_BACKEND_URL || 'http://localhost:3001',
  
  endpoints: {
    lmstudio: {
      health: '/api/lmstudio/health',
      models: '/api/lmstudio/models',
      chat: '/api/lmstudio/chat/completions',
      detectEndpoint: '/api/lmstudio/detect-endpoint'
    }
  }
};
```

##### Task 4.2 : Mise √† jour lmStudioService.ts
```typescript
// services/lmStudioService.ts - MODIFIER

import { API_CONFIG } from '../config/api.config';

// Remplacer toutes les occurrences de fetch direct vers LMStudio
// AVANT:
// const response = await fetch(`${endpoint}/v1/models`);

// APR√àS:
const BACKEND_URL = API_CONFIG.BACKEND_URL;
const response = await fetch(
  `${BACKEND_URL}${API_CONFIG.endpoints.lmstudio.models}?endpoint=${encodeURIComponent(endpoint)}`
);

// Pour detectAvailableEndpoint():
export const detectAvailableEndpoint = async (): Promise<string> => {
  const response = await fetch(`${BACKEND_URL}${API_CONFIG.endpoints.lmstudio.detectEndpoint}`);
  const data = await response.json();
  return data.endpoint;
};

// Pour checkServerHealth():
export const checkServerHealth = async (endpoint?: string): Promise<LMStudioHealthResponse> => {
  const url = endpoint 
    ? `${BACKEND_URL}${API_CONFIG.endpoints.lmstudio.health}?endpoint=${encodeURIComponent(endpoint)}`
    : `${BACKEND_URL}${API_CONFIG.endpoints.lmstudio.health}`;
  
  const response = await fetch(url);
  return await response.json();
};

// Pour generateContentStream():
export const generateContentStream = async function* (
  endpoint: string,
  model: string,
  systemInstruction?: string,
  history?: ChatMessage[],
  tools?: Tool[],
  outputConfig?: OutputConfig,
  apiKey?: string
) {
  const messages = formatMessages(history, systemInstruction);
  
  const response = await fetch(`${BACKEND_URL}${API_CONFIG.endpoints.lmstudio.chat}`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      endpoint,
      model,
      messages,
      stream: true,
      temperature: 0.1,
      max_tokens: 4000,
      tools: tools?.map(t => ({ type: 'function', function: t }))
    })
  });
  
  if (!response.ok) {
    throw new Error(`Backend proxy error: ${response.status}`);
  }
  
  const reader = response.body?.getReader();
  // ... reste du code streaming inchang√©
};
```

##### Task 4.3 : Variables d'environnement
```bash
# .env - CR√âER/MODIFIER
VITE_BACKEND_URL=http://localhost:3001
```

#### 4.2 Tests E2E
1. [ ] D√©marrer backend (`npm run dev` dans backend/)
2. [ ] D√©marrer frontend (`npm run dev` dans root/)
3. [ ] D√©marrer LMStudio (port 1234)
4. [ ] Settings ‚Üí LMStudio ‚Üí D√©tecter capacit√©s
5. [ ] Cr√©er agent LMStudio
6. [ ] Envoyer message et v√©rifier streaming

#### 4.3 Crit√®res validation
- [ ] ‚úÖ Aucune requ√™te directe vers LMStudio depuis frontend
- [ ] ‚úÖ Toutes les requ√™tes passent par backend proxy
- [ ] ‚úÖ Streaming fonctionne
- [ ] ‚úÖ D√©tection mod√®les fonctionnelle
- [ ] ‚úÖ Chat agent op√©rationnel
- [ ] ‚úÖ Aucune erreur CORS
- [ ] ‚úÖ Tests E2E r√©ussis

---

### üîµ Jalon 5 : Optimisations (1-2h)

**Objectif** : Cache, retry, m√©triques

#### 5.1 T√¢ches

##### Task 5.1 : Cache mod√®les
```typescript
// services/lmstudioProxy.service.ts - Ajouter
interface CacheEntry {
  data: LMStudioModelsListResponse;
  timestamp: number;
}

const modelsCache = new Map<string, CacheEntry>();

export async function fetchLMStudioModelsWithCache(endpoint: string): Promise<LMStudioModelsListResponse> {
  const cacheKey = endpoint;
  const cached = modelsCache.get(cacheKey);
  
  if (cached && Date.now() - cached.timestamp < LMSTUDIO_CONFIG.MODELS_CACHE_TTL_MS) {
    console.log('[LMStudio] Using cached models for', endpoint);
    return cached.data;
  }
  
  const models = await fetchLMStudioModels(endpoint);
  modelsCache.set(cacheKey, { data: models, timestamp: Date.now() });
  
  return models;
}
```

##### Task 5.2 : Retry logic
```typescript
// services/lmstudioProxy.service.ts - Ajouter
export async function fetchWithRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = LMSTUDIO_CONFIG.MAX_RETRIES,
  delay: number = LMSTUDIO_CONFIG.RETRY_DELAY_MS
): Promise<T> {
  let lastError: Error | undefined;
  
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error instanceof Error ? error : new Error('Unknown error');
      console.warn(`[LMStudio] Attempt ${attempt}/${maxRetries} failed:`, lastError.message);
      
      if (attempt < maxRetries) {
        await new Promise(resolve => setTimeout(resolve, delay * attempt));
      }
    }
  }
  
  throw lastError;
}
```

##### Task 5.3 : M√©triques
```typescript
// services/metrics.service.ts - CR√âER
interface Metrics {
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  averageResponseTime: number;
  cacheHits: number;
  cacheMisses: number;
}

const metrics: Metrics = {
  totalRequests: 0,
  successfulRequests: 0,
  failedRequests: 0,
  averageResponseTime: 0,
  cacheHits: 0,
  cacheMisses: 0
};

export function recordRequest(success: boolean, responseTime: number) {
  metrics.totalRequests++;
  if (success) metrics.successfulRequests++;
  else metrics.failedRequests++;
  
  // Moving average
  metrics.averageResponseTime = 
    (metrics.averageResponseTime * (metrics.totalRequests - 1) + responseTime) / metrics.totalRequests;
}

export function recordCacheHit(hit: boolean) {
  if (hit) metrics.cacheHits++;
  else metrics.cacheMisses++;
}

export function getMetrics(): Metrics {
  return { ...metrics };
}

export function resetMetrics() {
  Object.keys(metrics).forEach(key => {
    (metrics as any)[key] = 0;
  });
}
```

##### Task 5.4 : Route m√©triques
```typescript
// routes/lmstudio.routes.ts - Ajouter
import { getMetrics, resetMetrics } from '../services/metrics.service';

router.get('/metrics', (req: Request, res: Response) => {
  res.json(getMetrics());
});

router.post('/metrics/reset', (req: Request, res: Response) => {
  resetMetrics();
  res.json({ message: 'Metrics reset successfully' });
});
```

#### 5.2 Crit√®res validation
- [ ] ‚úÖ Cache mod√®les 10min fonctionnel
- [ ] ‚úÖ Retry logic 3 tentatives appliqu√©
- [ ] ‚úÖ M√©triques collect√©es
- [ ] ‚úÖ Route `/metrics` accessible
- [ ] ‚úÖ Performance v√©rifi√©e (< 500ms fetch models avec cache)

---

## üìä M√âTRIQUES DE SUCC√àS

### Performance
- ‚è±Ô∏è Health check : < 2s
- ‚è±Ô∏è Fetch models (cache hit) : < 50ms
- ‚è±Ô∏è Fetch models (cache miss) : < 500ms
- ‚è±Ô∏è Streaming latency : < 100ms

### S√©curit√©
- üîí 0 requ√™te directe frontend ‚Üí LMStudio
- üîí Rate limiting actif (60 req/min)
- üîí Validation requ√™tes 100%
- üîí Whitelist endpoints respect√©e

### Stabilit√©
- ‚úÖ 0 erreur CORS
- ‚úÖ Retry logic fonctionnel
- ‚úÖ Gestion erreurs robuste
- ‚úÖ Logs complets sans fuite donn√©es

---

## üîç TESTS DE NON-R√âGRESSION

Apr√®s impl√©mentation compl√®te, v√©rifier :

### Frontend
- [ ] ‚úÖ Cr√©ation agent LMStudio
- [ ] ‚úÖ Chat avec agent LMStudio
- [ ] ‚úÖ Streaming r√©ponses
- [ ] ‚úÖ D√©tection mod√®les dynamiques
- [ ] ‚úÖ Settings ‚Üí LMStudio ‚Üí D√©tection capacit√©s
- [ ] ‚úÖ Function calling (si mod√®le supporte)

### Backend
- [ ] ‚úÖ Route Python tools inchang√©e
- [ ] ‚úÖ WebSocket fonctionnel
- [ ] ‚úÖ CORS existant pr√©serv√©

### Autres Providers
- [ ] ‚úÖ Gemini fonctionne
- [ ] ‚úÖ OpenAI fonctionne
- [ ] ‚úÖ Anthropic fonctionne
- [ ] ‚úÖ Mistral API fonctionne

---

## üìù DOCUMENTATION √Ä METTRE √Ä JOUR

Apr√®s impl√©mentation :
1. [ ] README.md - Section "LMStudio Setup"
2. [ ] ARCHITECTURE.md - Diagramme proxy
3. [ ] API.md - Documentation endpoints backend
4. [ ] SECURITY.md - Politiques s√©curit√© proxy

---

## üöÄ COMMANDES UTILES

### D√©veloppement
```powershell
# Backend
cd backend
npm run dev

# Frontend
npm run dev

# Tests backend
cd backend
npm test

# Logs backend
# Dans backend/src/server.ts, activer debug mode
```

### Tests Manuels
```powershell
# Health check
curl http://localhost:3001/api/lmstudio/health

# Models
curl http://localhost:3001/api/lmstudio/models

# Detect endpoint
curl http://localhost:3001/api/lmstudio/detect-endpoint

# Chat (streaming)
curl -X POST http://localhost:3001/api/lmstudio/chat/completions `
  -H "Content-Type: application/json" `
  -d '{"model":"mistral-7b","messages":[{"role":"user","content":"Hello"}]}'

# Metrics
curl http://localhost:3001/api/lmstudio/metrics
```

---

## ‚úÖ CHECKLIST FINALE

- [ ] ‚úÖ Jalon 1 compl√©t√© et valid√©
- [ ] ‚úÖ Jalon 2 compl√©t√© et valid√©
- [ ] ‚úÖ Jalon 3 compl√©t√© et valid√©
- [ ] ‚úÖ Jalon 4 compl√©t√© et valid√©
- [ ] ‚úÖ Jalon 5 compl√©t√© et valid√©
- [ ] ‚úÖ Tests E2E passent
- [ ] ‚úÖ Tests non-r√©gression passent
- [ ] ‚úÖ Documentation mise √† jour
- [ ] ‚úÖ Code review effectu√©
- [ ] ‚úÖ Merge vers main

---

## üéØ STATUT ACTUEL : JALON 4 EN COURS

### ‚úÖ Jalons 1-3 : COMPL√âT√âS
- **Jalon 1** : Backend foundation ‚úÖ
- **Jalon 2** : Streaming SSE ‚úÖ
- **Jalon 3** : S√©curit√© (validation, rate limiting, logging) ‚úÖ
- **Bug fixes** : IPv6 error ‚úÖ, Port conflict ‚úÖ

### üîµ Jalon 4 : Migration Frontend EN COURS

#### Modifications effectu√©es
1. **config/api.config.ts** - CR√â√â
   - Configuration centralis√©e des endpoints backend
   - Fonction `buildBackendUrl()` pour construire URLs compl√®tes
   - Fonction `buildLMStudioProxyUrl()` pour routes LMStudio

2. **vite-env.d.ts** - CR√â√â
   - Types TypeScript pour `import.meta.env.VITE_BACKEND_URL`

3. **.env** - CR√â√â
   - `VITE_BACKEND_URL=http://localhost:3001`

4. **services/routeDetectionService.ts** - MODIFI√â
   - `detectLMStudioModel()` : Utilise `/api/lmstudio/detect-endpoint` au lieu d'appeler LMStudio directement
   - `testRoute()` : Pour `/v1/models`, utilise `/api/lmstudio/models` via backend proxy
   - Plus d'appels directs vers LMStudio depuis le frontend

#### Prochaines modifications n√©cessaires
- [ ] Mettre √† jour `services/lmStudioService.ts` pour router tous les appels via backend
- [ ] Red√©marrer frontend avec nouvelle configuration
- [ ] Tests E2E : Settings ‚Üí D√©tecter LMStudio

**Prochaine √©tape**: Tester la d√©tection LMStudio depuis l'interface Settings
